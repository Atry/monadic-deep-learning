\section{Introduction}
\label{introduction}

Backpropagation \cite{rumelhart1985learning} is the key feature in many deep learning frameworks. Combining with other optimization algorithms \cite{kingma2014adam, zeiler2012adadelta,duchi2011adaptive}, deep learning frameworks change the values of trainable variables in neural networks during iterations, producing a model of knowledge learnt from training data.

Backpropagation can be seen as a special-purpose Automatic Differentiation (AD) \cite{baydin2015automatic}. Many successful Python deep learning frameworks \cite{tokui2015chainer,google2017eager,paszke2017pytorch,neubig2017dynet} implement a common set of features of auto differentiation:

\begin{description}

  \item[Reverse mode] All these deep learning frameworks perform reverse mode AD instead of forward mode, as forward mode AD does not scale well for deep neural networks.

  \item[Multiple trainable variable] Neural networks are composed of multiple layers. Each layer contains their own trainable variables. All these deep learning frameworks are able to calculus the derivatives of all trainable variables at once for one training data batch.

  \item[Internal DSL \cite{fowler2010domain}] All these deep learning frameworks are libraries that provide an Internal DSL, allowing users to create their differentiable functions in Python or Lua from the similar expression as creating ordinary non-differentiable functions. Since these frameworks do not require external language, models created by them can be easy integrated into a larger application alone with higher-level configurations \cite{chollet2015keras} or ETL (Extract, Transform and Load) process.

\end{description}

Unfortunately, deep learning frameworks in statically typed languages have not achieved the above goals until 2017.

Several AD libraries \cite{bischof1992adifor,griewank1996algorithm,TapenadeRef13,baydin2015diffsharp} written in Fortran, C++ or F\# support AD via operator overloading or external preprocessor but do not support multiple trainable variables.

Other deep learning frameworks in statically typed language (including Scala binding of C++ frameworks) \cite{intel2016bigdl,skymind2017deeplearning4j,baydin2016hype,chen2017typesafe,zhao2017deepdsl} do not support AD, instead, they only provide their high level \gls{computational graph} APIs to compose predefined layers into neural networks. As a result, those frameworks do not have the ability to create fine-grained custom algorithms.

In this paper, we present DeepLearning.scala, which achieves all the above goals, and still get type checked.

\section{Basic Concepts}
\label{concepts}

For example, suppose we are building a robot for answering questions in IQ test like this:

\begin{quote}
  What is the next number in sequence:
    \begin{quote}
    3, 6, 9, ?
    \end{quote}
  The answer is 12.
\end{quote}

In DeepLearning.scala, the robot can be implemented as a \lstinline{guessNextNumber} function of the following signature\footnote{The code examples from Listing~\ref{guessNextNumber} to Listing~\ref{predict_trained} do not contain necessary \lstinline{import} and configurations. For a runnable IQ test robot example backed by ND4J \cite{skymind2017nd4j}, see \href{http://deeplearning.thoughtworks.school/demo/GettingStarted.html}{Getting Started documentation on DeepLearning.scala website}.}:

\begin{lstlisting}[float={h t b p},caption={The differentiable matrix multiplication implemented by \lstinline{map}/\lstinline{reduce}},label={guessNextNumber}]
def guessNextNumber(question: Seq[Double]): DoubleLayer = {
  // Perform a matrix multiplication between question and weights
  (question zip weights).map {
    case (element, weight) => element * weight
  }.reduce(_ + _) + bias
}
\end{lstlisting}

\lstinline{guessNextNumber} performs a matrix multiplication between question and weights by invoking higher-order functions \lstinline{map} and \lstinline{reduce}. 

Unlike \cite{chen2017typesafe}'s special tensor type, our tensor can be simply typed as \lstinline{Seq[Double]}, \lstinline{Seq[DoubleWeight]} or \lstinline{Seq[DoubleLayer]}.
\footnote{DeepLearning.scala users can use other representations of tensors:
\begin{enumerate*}
  \item For tensors with a statically typed shape, use \href{https://javadoc.io/page/com.chuusai/shapeless_2.11/latest/shapeless/Sized.html}{ \lstinline{shapeless.Sized}}. For example a $10\times20$ two-dimensional tensor can be typed as \lstinline{Sized[Sized[Double, _10], _20]}. For differentiable tensors, replace vanilla \lstinline{Double} to \lstinline{DoubleWeight} or \lstinline{DoubleLayer}.
  \item For GPU-accelerated tensors, use \lstinline{INDArray} \cite{skymind2017nd4j}. For differentiable tensors, use \lstinline{INDArrayLayer} or \lstinline{INDArrayWeight} instead.
\end{enumerate*}
}

The \lstinline{weights} and \lstinline{bias} referenced by \lstinline{guessNextNumber} must be initialized first:

\begin{lstlisting}[float={h t b p},caption={Weight initialization}]
val weights: Seq[DoubleWeight] = Stream.continually(DoubleWeight(math.random))
val bias: DoubleWeight = DoubleWeight(math.random)
\end{lstlisting}

Then the robot try to answer IQ test questions like calling an ordinary function:

\begin{lstlisting}[float={h t b p},caption={Inference on an untrained model}]
val question = Seq(42.0, 43.0, 44.0)
println(guessNextNumber(question).predict.blockingAwait)
\end{lstlisting}

However, \lstinline{guessNextNumber} returned an incorrect result because the weights and bias were randomly initialized, which have not been trained.

In order to train them, a loss function is necessary:

\begin{lstlisting}[float={h t b p},caption={The differentiable square loss function},label={squareLoss}]
def squareLoss(robotAnswer: DoubleLayer, expectedAnswer: Double): DoubleLayer = {
  val difference: DoubleLayer = robotAnswer - expectedAnswer
  difference * difference
}
\end{lstlisting}

The above loss function \lstinline{squareLoss} determines the squared error between robot's answer and the correct answer.

Both \lstinline{squareLoss} and \lstinline{guessNextNumber} are ordinary functions, and can be composed in other functions:

\begin{lstlisting}[float={h t b p},caption={A differentiable function to train a linear regression model}]
def linearRegression(question: Seq[Double], expectedAnswer: Double): DoubleLayer = {
  val robotAnswer = guessNextNumber(question)
  squareLoss(robotAnswer, expectedAnswer)
}
\end{lstlisting}

\lstinline{linearRegression}, composed of \lstinline{squaredLoss} and \lstinline{squaredLoss}, returns a \lstinline{DoubleLayer} of the loss for a specific question and its expected answer. \lstinline{linearRegression} is a linear regression model with a square loss, and it can be trained.

\begin{lstlisting}[float={h t b p},caption={Training for 500 iterations}]
val question1 = Seq(3.0, 4.0, 5.0)
val expectedAnswer1 = 6.0

val question2 = Seq(13.0, 19.0, 25.0)
val expectedAnswer2 = 31.0

for (iteration <- 0 until 500) {
  linearRegression(question1, expectedAnswer1).train.blockingAwait
  linearRegression(question2, expectedAnswer2).train.blockingAwait
}
\end{lstlisting}

The weights and bias referenced by \lstinline{linearRegression} are modified during 500 iterations of training, toward the direction to minimize the loss returned from \lstinline{linearRegression}.

When weights and bias have been adjusted to make the loss a very small number, \lstinline{guessNextNumber} should return values that are very close to the expected answers.

\begin{lstlisting}[float={h t b p},caption={Inference on a trained model},label={predict_trained}]
val question = Seq(42.0, 43.0, 44.0)
println(guessNextNumber(question).predict.blockingAwait)
\end{lstlisting}

This time, it will prints a number closed to 45, because the IQ test robot have finally learnt the pattern of arithmetic progression.

The IQ test robot example shows some basic concepts in DeepLearning.scala.

\begin{itemize}
  \item \lstinline{guessNextNumber}, \lstinline{squareLoss} and \lstinline{linearRegression} are \glspl{differentiable function} that return \glspl{differentiable expression}, which are \gls{computational graph} nodes that can be evaluated when \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html#train(differentiable:Differentiable)(implicitmonoid:algebra.ring.MultiplicativeMonoid[DeepLearning.this.Delta]):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{train}}ing or \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html#predict(differentiable:Differentiable):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{predict}}ing.
  \item \Glspl{differentiable expression} and \glspl{trainable variable} can be used as if they are ordinary non-differentiable values. For example, as shown in Listing~\ref{squareLoss}, you can perform scalar subtraction and multiplication between \lstinline{DoubleWeight}, \lstinline{DoubleLayer} and ordinary \lstinline{scala.Double}.
  \item When \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html#train(differentiable:Differentiable)(implicitmonoid:algebra.ring.MultiplicativeMonoid[DeepLearning.this.Delta]):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{train}}ing a \gls{differentiable expression}, it returns a \href{https://javadoc.io/page/com.thoughtworks.future/future_2.11/latest/com/thoughtworks/future%24%24Future.html}{\lstinline{Future}}, which encapsulates the side-effect of adjusting \glspl{trainable variable} referenced by the \gls{differentiable function}.
  \item If a \gls{differentiable function} invokes another \gls{differentiable function}, then \glspl{trainable variable} trained by one \gls{differentiable function} affect another one. For example, when training the \gls{differentiable function} \lstinline{linearRegression}, The \glspl{trainable variable} \lstinline{weights} and \lstinline{bias} are modified, hence \lstinline{guessNextNumber} automatically gains the ability to \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html#predict(differentiable:Differentiable):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{predict}} correct answers.
\end{itemize}

\section{Dynamic Neural Networks}

DeepLearning.scala supports dynamic neural network. It means that the control flow of a neural network can differ according to its internal intermediate state when processing a special input. Especially, the ability to conditional enabling a sub-neural network is a crucial feature to build outrageously large neural networks \cite{shazeer2017outrageously}.

Suppose we have two sub-neural networks, \lstinline{leftSubnet} and \lstinline{rightSubnet}. We want to build a gated network, which conditionally runs either \lstinline{leftSubnet} or \lstinline{rightSubnet} for a special \lstinline{input}.

\begin{lstlisting}[float={h t b p},caption={Predefined sub-networks}]
def leftSubnet(input: INDArrayLayer): INDArrayLayer
def rightSubnet(input: INDArrayLayer): INDArrayLayer
\end{lstlisting}

Which sub-network is selected for the \lstinline{input} should be determined by the \lstinline{gate} network, which returns a pair of differentiable double expressions that indicate the preferences between \lstinline{leftSubnet} and \lstinline{rightSubnet}.

\begin{lstlisting}[float={h t b p},caption={Predefined gate network}]
def gate(input: INDArrayLayer): (DoubleLayer, DoubleLayer)
\end{lstlisting}

The control flow of gated network that we want to build is described in Function~\ref{GatedNetwork}.

\begin{function}[H]
  \caption{GatedNetwork()\label{GatedNetwork}}
  \KwIn{Features extracted by preceding layers}
  \KwOut{Features passed to succeeding layers}
  \SetKwFunction{gate}{gate}
  \SetKwFunction{leftSubnet}{leftSubnet}
  \SetKwFunction{rightSubnet}{rightSubnet}
  scores $\leftarrow$ \gate{Input}\;
  \eIf{score of left sub-network $>$ score of right sub-network}{
      \KwRet{score of left sub-network $\times$ \leftSubnet{Input}}\;
  }{
      \KwRet{score of right sub-network $\times$ \rightSubnet{Input}}\;
  }
\end{function}

In DeepLearning.scala, there are three different approaches to implement the gated network. Examples of these approaches are introduced in following Section~\ref{eager}, Section~\ref{monadic}, and Section~\ref{applicative}.

\subsection{Eager Execution (bad)}
\label{eager}

An obvious approach to create the gated network is eagerly executing the \lstinline{gate}, shown in Listing~\ref{naiveGatedNet}:

\begin{lstlisting}[float={h t b p},caption={The eager execution implementation of gated network}, label={naiveGatedNet}]
def naiveGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  if (scores._1.predict.blockingAwait > scores._2.predict.blockingAwait) {
    scores._1 * leftSubnet(input)
  } else {
    scores._2 * rightSubnet(input)
  }
}
\end{lstlisting}

There are three sub-networks in the \lstinline{naiveGatedNet} function. The \lstinline{gate} returns a pair of \lstinline{DoubleLayer}s. By blocking await the \lstinline{predict}ion result, we got two \lstinline{Double}s, which can be used to determine which sub-network is preferred between \lstinline{leftSubnet} and \lstinline{rightSubnet}. The chosen sub-network will multiplies with the value returned by the \lstinline{gate} in order to enabling backpropagation on the \lstinline{gate}.

However, there is a performance issue in the \lstinline{naiveGatedNet}.

In DeepLearning.scala, all differentiable expressions, including the scalar \lstinline{DoubleLayer} and vectorize \lstinline{INDArrayLayer}, contain some lazily evaluated differentiable \gls{computational graph} nodes, which will not be executed until their predict or train methods are invoked.

So, the two calls to the \lstinline{predict} method in the \lstinline{if} will execute the \gls{computational graph} in \lstinline{gate} twice. Also the \gls{computational graph} in \lstinline{naiveGatedNet} will be executed when users call \lstinline{predict} or \lstinline{train} call \lstinline{naiveGatedNet} in the future. But what's even worse is, \lstinline{input} contains a \gls{computational graph}, too. Along with \lstinline{gate}, it will be evaluated three times, which may contain complex future extracting process.

\subsection{Monadic Control Flow (good)}
\label{monadic}

Ideally, the calls to \lstinline{predict} should be avoided in differentiable functions. The recommended approach to create a dynamic neural network is using \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html#forward(differentiable:Differentiable):com.thoughtworks.raii.asynchronous.Do[com.thoughtworks.deeplearning.DeepLearning.Tape[DeepLearning.this.Data,DeepLearning.this.Delta]]}{\lstinline{forward}}, which returns a monadic value of \lstinline{Do[Tape[Data, Delta]]}, which can be used in a monadic control flow via Scalaz \cite{kenji2017scalaz}'s type classes \cite{oliveira2010type} \lstinline{Monad} and \lstinline{Applicative}.

Listing~\ref{monadicGatedNet} shows the monadic control flow of gated network.

\begin{lstlisting}[float={h t b p},caption={Monadic gated network}, label={monadicGatedNet}]
def monadicGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  val gatedForward: Do[Tape[INDArray, INDArray]] = {
    scores._1.forward.flatMap { tape1: Tape[Double, Double] =>
      scores._2.forward.flatMap { tape2: Tape[Double, Double] =>
        if (tape1.data > tape2.data) {
          (scores._1 * leftSubnet(input)).forward
        } else {
          (scores._2 * rightSubnet(input)).forward
        }
      }
    }
  }
  INDArrayLayer(gatedForward)
}
\end{lstlisting}

This gated network is built from the monadic expression \lstinline{gatedForward}, which contains some \lstinline{forward} calls, which are asynchronous operations (or \href{https://javadoc.io/page/com.thoughtworks.raii/asynchronous_2.11/latest/com/thoughtworks/raii/asynchronous%24%24Do.html}{\lstinline{Do}}) that produce Wengert list record (or \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning%24%24Tape.html}{\lstinline{Tape}}). The implementation detail of \lstinline{Do} and \lstinline{Tape} will be discussed in Section~\ref{implementation}. For now, we only need to know that \lstinline{Do} is a monadic data type that supports \lstinline{flatMap}. By \lstinline{flatMap}ping those \lstinline{forward} operations together, we built the entire monadic control flow \lstinline{gatedForward} for the gated network.

The \lstinline{monadicGatedNet} represents a dynamic neural network, since each \lstinline{forward} operation is started after its previous \lstinline{forward} done. This behavior allows dynamically determining succeeding operations according to result of previous \lstinline{forward} operations, as shown in the \lstinline{if} clause in Listing~\ref{monadicGatedNet}.

However, \lstinline{flatMap} prevents additional optimization, too.
\lstinline{scores._2.forward} have to wait for \lstinline{scores._1.forward}'s result, even if the two operations are logically independent.

\subsection{Parallel Applicative Control Flow + Sequential Monadic Control Flow (best)}
\label{applicative}

Ideally, the independent operations \lstinline{scores._1.forward} and \lstinline{scores._2.forward} should run in parallel. This can be done by tagging \lstinline{Do} as \href{https://javadoc.io/page/org.scalaz/scalaz_2.11/latest/scalaz/Tags%24%24Parallel.html}{\lstinline{Parallel}}, and use \href{https://javadoc.io/page/org.scalaz/scalaz_2.11/latest/scalaz/Applicative.html#tuple2[A,B](fa:=>F[A],fb:=>F[B]):F[(A,B)]}{\lstinline{scalaz.Applicative.tuple2}} instead of \lstinline{flatMap} (Listing~\ref{applicativeMonadicGatedNet}).

\begin{lstlisting}[float={h t b p},caption={Applicative + monadic gated network}, label={applicativeMonadicGatedNet}]
def applicativeMonadicGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  val parallelForward1: ParallelDo[Tape[Double, Double]] = {
    Parallel(scores._1.forward)
  }
  val parallelForward2: ParallelDo[Tape[Double, Double]] = {
    Parallel(scores._2.forward)
  }
  val Parallel(stage1) = {
  	parallelForward1.tuple2(parallelForward2)
  }
  def stage2(tapes: (Tape[Double, Double], Tape[Double, Double])) = {
    if (tapes._1.data > tapes._2.data) {
      (scores._1 * leftSubnet(input)).forward
    } else {
      (scores._2 * rightSubnet(input)).forward
    }
  }

  val gatedForward = stage1.flatMap(stage2)
  INDArrayLayer(gatedForward)
}
\end{lstlisting}

This \lstinline{applicativeMonadicGatedNet} takes both advantages from applicative functors and monads. The entire control flow is a \lstinline{flatMap} sequentially composed of two stages. In \lstinline{stage1}, there is a \lstinline{tuple2} composed of \lstinline{scores._1.forward} and \lstinline{scores._2.forward} in parallel. Then, in \lstinline{stage2}, the succeeding operation is dynamically determined according to \lstinline{tapes}, the result of \lstinline{stage1}.

The parallel applicative operation is also the default behavior for all built-in vector binary operators. Listing~\ref{parallelByDefault} shows some simple expressions that will be executed in parallel.

\begin{lstlisting}[float={h t b p},caption={By default, \lstinline{a * b} and \lstinline{c * d} will be executed in parallel because they are independent}, label={parallelByDefault}]
def parallelByDefault(a: INDArrayLayer, b: INDArrayLayer, c: INDArrayLayer, d: INDArrayLayer): INDArrayLayer = {
  a * b + c * d
}
\end{lstlisting}

By combining both applicative functors and monads, DeepLearning.scala supports dynamic neural network and still allows the independent parts of the neural network to run in parallel. In addition, the backward() pass of differentiable functions built from parallel applicative functors or built-in vector binary operators will be executed in parallel, too.

\section{Ad Hoc Polymorphic Differentiable Functions}
\label{Ad Hoc Polymorphism}

Neural networks created in DeepLearning.scala are \glspl{differentiable function}, which contain expressions of differentiable types, which are any types that has their corresponding \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html}{\lstinline{DeepLearning}} type classes, including:

\begin{itemize}
  
  \item A vanilla vector input. i.e. \lstinline{INDArray}.

  \item \Glspl{differentiable expression} of hidden states produced by any previous neural network layers, i.e. any \lstinline{INDArrayLayer}s regardless of the prefixes.
  
  \item \Glspl{trainable variable} in the case of activation maximization technique \cite{erhan2009visualizing}.i.e. any \lstinline{INDArrayWeight}s regardless of the prefixes.

\end{itemize}

Table~\ref{differentiable types} shows nine types that have built-in \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html}{\lstinline{DeepLearning}} type classes.

\begin{table}[h t b p]
  \caption{Built-in Differentiable Types}
  \label{differentiable types}
  \begin{tabular}{c l l l}
    \toprule
    & non-trainable value & \gls{trainable variable} & \gls{differentiable expression} \\
    \midrule
    single-precision scalar & \href{https://www.scala-lang.org/api/current/scala/Double.html}{\lstinline$Double$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-doubleweights_2.11/latest/com/thoughtworks/deeplearning/plugins/DoubleWeights%24DoubleWeight.html}{\lstinline$DoubleWeight$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-doublelayers_2.11/latest/com/thoughtworks/deeplearning/plugins/DoubleLayers%24DoubleLayer.html}{\lstinline$DoubleLayer$} \\
    double-precision scalar & \href{https://www.scala-lang.org/api/current/scala/Float.html}{\lstinline$Float$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-floatweights_2.11/latest/com/thoughtworks/deeplearning/plugins/FloatWeights%24FloatWeight.html}{\lstinline$FloatWeight$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-floatlayers_2.11/latest/com/thoughtworks/deeplearning/plugins/FloatLayers%24FloatLayer.html}{\lstinline$FloatLayer$} \\
    vector & \href{https://nd4j.org/doc/org/nd4j/linalg/api/ndarray/INDArray.html}{\lstinline$INDArray$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-indarrayweights_2.11/latest/com/thoughtworks/deeplearning/plugins/INDArrayWeights%24INDArrayWeight.html}{\lstinline$INDArrayWeight$} & \href{https://javadoc.io/page/com.thoughtworks.deeplearning/plugins-indarraylayers_2.11/latest/com/thoughtworks/deeplearning/plugins/INDArrayLayers%24INDArrayLayer.html}{\lstinline$INDArrayLayer$} \\
    \bottomrule
  \end{tabular}
\end{table}

Ideally, a differentiable function should be an ad hoc polymorphic function that accepts heterogeneous types of input.

Our solution is the dependent-type type class \cite{gurnelltype} \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html}{\lstinline{DeepLearning}} that witnesses any supported expressions including \glspl{differentiable expression}, \glspl{trainable variable}, or vanilla non-differentiable types. The users can create type aliases to restrict the types of state during forward pass and backward pass as shown in Listing~\ref{INDArrayDeepLearning}.

\begin{lstlisting}[float={h t b p},caption={A type class alias that witnesses dense vector expressions}, label={INDArrayDeepLearning}]
type INDArrayExpression[Expression] = DeepLearning[Expression] {
  /** The type of result calculated during forward pass */
  type Data = INDArray

  /** The type of derivative during backward() pass */
  type Delta = INDArray
}
\end{lstlisting}

By using \lstinline{INDArrayExpression} as a context bound, we can create a polymorphic differentiable function that accepts any vector expression.

\begin{lstlisting}[float={h t b p},caption={A polymorphic differentiable function}, label={polymorphicDifferentiableFunction}]
def polymorphicDifferentiableFunction[A: INDArrayExpression, B: INDArrayExpression, C: INDArrayExpression, D: INDArrayExpression](a: A, b: B, c: C, d: D): INDArrayLayer = {
  a * b + c * d
}
\end{lstlisting}

Listing~\ref{polymorphicDifferentiableFunction} is similar to Listing~\ref{parallelByDefault}, except each argument of \lstinline{polymorphicDifferentiableFunction} accepts \lstinline{INDArray}, \lstinline{INDArrayWeight} or \lstinline{INDArrayLayer} respectively, not only  \lstinline{INDArrayLayer}.

Note that built-in operations including arithmetic operations, \lstinline{max}, and \lstinline{dot} are polymorphic differentiable functions, too, which can be used in user-defined polymorphic \glspl{differentiable function}.

\section{Implementation}
\label{implementation}

In this section, we will introduce the internal data structure used in DeepLearning.scala to perform AD.

\begin{itemize}

  \item For ease of understanding, Section~\ref{dual number} starts from a simple dual number implementation \lstinline{DualNumber}, which was known as an approach to perform forward mode AD for scalar values.
  
  \item Section~\ref{monadic dual number} introduces our variation of dual number \lstinline{ClosureBasedDualNumber}, which supports tree-structured reverse mode AD (aka backpropagation) for multiple \glspl{trainable variable}.

  \item Section~\ref{generic tape} shows the actual data type \lstinline{Tape} in DeepLearning.scala, which is generalized to not only scalar types, but also vector types and any other differentiable types.

  \item Section~\ref{reference counted tape} discovered the monadic control flow \lstinline{Do}, which manages the life circle of \lstinline{Tape}s, sharing \lstinline{Tape}s for common \gls{computational graph} nodes, allowing arbitrary DAG(Directed Acyclic Graph)-structured \gls{computational graph}.

  \item Section~\ref{training iteration} summarizes the entire execution process during a training iteration, showing how the user-defined \glspl{differentiable function} get executed through internal mechanisms \lstinline{Do} and \lstinline{Tape}.

\end{itemize}

\subsection{Ordinary Dual Number}
\label{dual number}

Our approach for reverse mode AD use a data structure similar to traditional forward mode AD, with only a few changes.

Forward mode AD can be viewed as computation on dual number. For example, dual number for scalar types can be implemented as Listing~\ref{DualNumber}:

\begin{lstlisting}[float={h t b p},caption={Dual number for forward mode AD}, label={DualNumber}]
type Data = Double
type PartialDelta = Double
case class DualNumber(data: Data, delta: PartialDelta)
\end{lstlisting}

Arithmetic operations on those dual number can be implemented as Listing~\ref{dualArithmetic}:

\begin{lstlisting}[float={h t b p},caption={Arithmetic operations on dual number}, label={dualArithmetic}]
object DualNumber {
  def plus(left: DualNumber, right: DualNumber): DualNumber = {
    DualNumber(left.data + right.data, left.delta + right.delta)
  }
  def multiply(left: DualNumber, right: DualNumber): DualNumber = {
    DualNumber(left.data * right.data, left.data * right.delta + right.data * left.delta)
  }
}
\end{lstlisting}

\subsection{Monadic Closure-based Dual Number}
\label{monadic dual number}

However, this approach is hard to type-check if we want to support multiple  \glspl{trainable variable}. \lstinline{PartialDelta} in Listing~\ref{DualNumber} represents the partial derivative of \glspl{trainable variable}. In AD tools that support only one \gls{trainable variable}, the \gls{trainable variable} is usually forced to be the input. Hence \lstinline{PartialDelta} is the input type for those AD tools. This assumption is broken for our case, since our delta type of a specific \lstinline{DualNumber} must contain derivatives for all \glspl{trainable variable} that were used to produce the \lstinline{DualNumber}, not only the partial derivative of input. As a result, the type of delta varies when the number of \glspl{trainable variable} grows.

To type-check the delta, considering the only usage of the delta in a neural network is updating \glspl{trainable variable} in a gradient descent based optimization algorithm. We can replace \lstinline{PartialDelta} to a \lstinline{UpdateWeights} closure.

\begin{lstlisting}[float={h t b p},caption={Replacing \lstinline{PartialDelta} to a closure}, label={class ClosureBasedDualNumber}]
type Data = Double  
case class ClosureBasedDualNumber(data: Data, backward: UpdateWeights)
\end{lstlisting}

\lstinline{UpdateWeights} in Listing~\ref{class ClosureBasedDualNumber} is a function type that contains side-effects to update \glspl{trainable variable}. In order to implement arithmetic operations for the new dual number, the operations on \lstinline{PartialDelta} should be replaced to custom functions for \lstinline{UpdateWeights} (Listing~\ref{object ClosureBasedDualNumber}):

\begin{lstlisting}[float={h t b p},caption={Replacing operations on \lstinline{PartialDelta} to custom functions for \lstinline{UpdateWeights}}, label={object ClosureBasedDualNumber}]  

object ClosureBasedDualNumber {
  def plus(left: ClosureBasedDualNumber, right: ClosureBasedDualNumber): ClosureBasedDualNumber = {
    ClosureBasedDualNumber(left.data + right.data, UpdateWeights.plus(left.backward(), right.backward()))
  }
  def multiply(left: ClosureBasedDualNumber, right: ClosureBasedDualNumber): ClosureBasedDualNumber = {
    ClosureBasedDualNumber(
      left.data * right.data,
      UpdateWeights.multiply(left.data, right.backward()) + UpdateWeights.multiply(right.data, left.backward()))
  }
}
\end{lstlisting}


Mathematically, the \lstinline{UpdateWeights} type in a dual number can be any vector space, i.e. the \lstinline{UpdateWeights} closure itself must support addition and scalar multiplication operations.

The addition operation for closures is defined as (\ref{differentiable addition}):

\begin{equation}
\label{differentiable addition}
(f_0 + f_1)(x) = f_0(x) + f_1(x)
\end{equation}

And the scalar multiplication operation for closures is defined as (\ref{differentiable multiplication}):

\begin{equation}
\label{differentiable multiplication}
(x_0f)(x_1) = f(x_0x_1)
\end{equation}

These arithmetic operations can be implemented in monadic data types as shown in Listing~\ref{UpdateWeights}.

\begin{lstlisting}[float={h t b p},caption={Arithmetic operations for the closure that contains side-effects}, label={UpdateWeights}, escapeinside={(*}{*)}]
type UpdateWeights = Do[Double] => SideEffects
object UpdateWeights {
  /**(* $(f_0 + f_1)(x) = f_0(x) + f_1(x)$ *)*/
  def plus(f0: UpdateWeights, f1: UpdateWeights) = { doX: Do[Double] =>
    f0(doX) |+| f1(doX)
  }

  /**(* $(x_0f)(x_1) = f(x_0x_1)$ *)*/
  def multiply(x0: Double, f: UpdateWeights) = { doX1: Do[Double] =>
    f(doX1.map(x0 * _))
  }
}
\end{lstlisting}

\lstinline{UpdateWeights}, as a replacement to original \lstinline{PartialDelta}, is a closure able to update derivatives for all weight with a coefficient (the \lstinline{Double} parameter).
\lstinline{|+|} is the \lstinline{append} operation of \lstinline{scala.Semigroup}, which could be any cumulative data type.

Also note that the parameter is a monadic data type \href{https://javadoc.io/page/com.thoughtworks.raii/asynchronous_2.11/latest/com/thoughtworks/raii/asynchronous%24%24Do.html}{\lstinline{Do}} that encapsulates the computation of derivative. Unlike strictly evaluated values, \lstinline{Do} is an operation evaluated in need.

In DeepLearning.scala, our \lstinline{SideEffects} is based on the asynchronous operation \lstinline{UnitContinuation}.

\begin{lstlisting}[float={h t b p},caption={Monadic side-effects}, label={SideEffects}]
type SideEffects = UnitContinuation[Unit]
\end{lstlisting}

\lstinline{UnitContinuation[A]} is an opaque alias \cite{erik2017opaque} of \lstinline{(A => Trampoline[Unit]) => Trampoline[Unit]}, implemented in a separate library at \href{https://github.com/ThoughtWorksInc/future.scala}{future.scala}. It is used in DeepLearning.scala as a monadic data type for encapsulating side effects in stack-safe asynchronous programming.

The \lstinline{SideEffects} for neural networks conform associative law because the only side effects is updating \glspl{trainable variable}. Thus, our \lstinline{UpdateWeights.plus} and \lstinline{UpdateWeights.multiply} are equivalent to the operations on strictly evaluated scalar value \lstinline{PartialDelta} in forward mode AD.

Since \lstinline{UpdateWeights} is a closure with side effects, a \gls{trainable variable} can be represented as a tuple of a mutable value and the action to modify the mutable value.

\begin{lstlisting}[float={h t b p},caption={Create a dual number for a \gls{trainable variable}}, label={createTrainableVariable}]
def createTrainableVariable(initialValue: Double, learningRate: Double): ClosureBasedDualNumber = {
  var data = initialValue
  val backward: UpdateWeights = { doDelta: Do[Double] =>
    val sideEffects: Do[Unit] = doDelta.map { delta =>
      value -= learningRate * delta
    }
    convertDoToUnitContinuation(sideEffects)
  }
  ClosureBasedDualNumber(data, backward)
}
\end{lstlisting}

In Listing~\ref{createTrainableVariable}, the \gls{trainable variable} is trained by a fixed learning rate to simplify the hyperparameters of optimization algorithms. The actual DeepLearning.scala implementation uses a more sophisticate approach to configure the hyperparameters  \begin{anonsuppress} described in \cite{yang2017expression}
\end{anonsuppress}
.

Similar to \glspl{trainable variable}, a non-trainable value can be represented as a tuple of the value and a no-op closure shown in Listing~\ref{createLiteral}.

\begin{lstlisting}[float={h t b p},caption={Create a dual number for a  non-trainable value}, label={createLiteral}]
def createLiteral(data: Double): ClosureBasedDualNumber = {
  val backward = { doDelta: Do[Double] =>
    UnitContinuation.now(())
  }
  ClosureBasedDualNumber(data, backward)
}
\end{lstlisting}

Because \lstinline{delta} is an action instead of pre-evaluated value, the implementation of \lstinline{backward} for non-trainable value can entirely avoid executing unnecessary computation in \lstinline{doDelta}.

Finally, we can create a differentiable function as shown in Listing~\ref{computationalTree}, whose leaf nodes are \lstinline{createTrainableVariable} and \lstinline{createLiteral}, and internal nodes are arithmetic operations in Listing~\ref{object ClosureBasedDualNumber}.

\begin{lstlisting}[float={h t b p},caption={A tree-structured \gls{differentiable function}},label={computationalTree}]
val w0 = createTrainableVariable(math.random, 0.001)
val w1 = createTrainableVariable(math.random, 0.001)

def computationalTree(x: ClosureBasedDualNumber) = {
  val y0 = ClosureBasedDualNumber.multiply(x, w0)
  val y1 = ClosureBasedDualNumber.multiply(y0, w1)
  y1
}
\end{lstlisting}

The \gls{computational graph} of \lstinline{computationalTree} is shown in Figure~\ref{tree}. Note that the arrow direction denotes the dependency between expressions, from arrow tail to arrow head, which is the reverse of the direction of data flow.

\begin{figure}[h t b p]

  \begin{dot2tex}[dot,mathmode]
  digraph {
    node [shape="circle"]
    rankdir=RL

    y_0 -> x
    y_0 -> w_0
    y_1 -> y_0
    y_1 -> w_1
    
  }
  \end{dot2tex}
    
  \caption{A tree-structured \gls{computational graph}}
  \label{tree}
\end{figure}

The closure-based dual number \lstinline{y1} has a closure \lstinline{backward}, which returns \lstinline{SideEffects} that recursively change all \glspl{trainable variable} referenced by the closure.

Note that \lstinline{backward} itself does not perform any side effects. It just collecting all side effects into a \lstinline{UnitContinuation[Unit]}. Figure~\ref{tree backpropagation} shows how the side effects of updating \glspl{trainable variable} are collected.

\begin{figure}[h t b p]
  \newcommand{\x}{$x$}
  \newcommand{\y}[1]{$y_#1$}
  \newcommand{\w}[1]{$w_#1$}
  
  \begin{sequencediagram}
    \newthread{Collecting side effects}{Collecting side effects}
    \newinst{y1}{\y1}
    \newinst{w1}{\w1}
    \newinst{y0}{\y0}
    \newinst{w0}{\w0}
    \newinst{x}{\x}
    \begin{call}{Collecting side effects}{\lstinline{y1.backward()}}{y1}{Updating \lstinline{w0.data} and \lstinline{w1.data}}
        \begin{call}{y1}{\lstinline{y0.backward()}}{y0}{Updating \lstinline{w0.data}}
          \begin{call}{y0}{\lstinline{x.backward()}}{x}{no-op}
          \end{call}
          \begin{call}{y0}{\lstinline{w0.backward()}}{w0}{Updating \lstinline{w0.data}}
          \end{call}
        \end{call}
        \begin{call}{y1}{\lstinline{w1.backward()}}{w1}{Updating \lstinline{w1.data}}
        \end{call}
    \end{call}
  \end{sequencediagram}

  \caption{Backpropagation for a tree-structured \gls{computational graph}}
  \label{tree backpropagation}
\end{figure}

Finally, the collected side effects of \lstinline{UnitContinuation[Unit]} returned from \lstinline{y1.backward} can be performed by a \href{https://javadoc.io/page/com.thoughtworks.future/future_2.11/latest/com/thoughtworks/continuation%24%24UnitContinuationOps.html#blockingAwait():A}{\lstinline{blockingAwait}} or \href{https://javadoc.io/page/com.thoughtworks.future/future_2.11/latest/com/thoughtworks/continuation%24%24ContinuationOps.html#onComplete(continue:A=>R):R}{\lstinline{onComplete}} call.

\subsection{Generic Tape}
\label{generic tape}

This closured-based monadic dual number can be generalized to any linear spaces, not only scalar types such as \lstinline{Double}, but also n-dimensional arrays.

The dual number type that we actually defined in DeepLearning.scala is \lstinline{Tape}, a generic version of \lstinline{ClosureBasedDualNumber} in Listing~\ref{class ClosureBasedDualNumber}. We replaced \lstinline{ClosureBasedDualNumber}'s hard-coded \lstinline{Double} to type parameters \lstinline{Data} and \lstinline{Delta}, as shown in Listing~\ref{Tape}.

\begin{lstlisting}[float={h t b p},caption={Generic closured-based monadic dual number}, label={Tape}]
final case class Tape[+Data, -Delta](
  data: Data,
  backward: Do[Delta] => UnitContinuation[Unit]
)
\end{lstlisting}

\lstinline{Data} and \lstinline{Delta} are usually the same, but they can also be different types. For example, you can create a type whose \lstinline{Data} is a dense n-dimensional array and whose {Delta} is a pair of index and scalar, representing a dense tensor that sparsely updates.

This data structure is similar to Wengert list in traditional reverse mode AD, except our tape is a tree of closures instead of a list.

\subsection{Reference Counted Tape}
\label{reference counted tape}

Although the closured-based dual number approach from Listing~\ref{class ClosureBasedDualNumber} to Listing~\ref{Tape} supports multiple \glspl{trainable variable}, the closure-based computation has a performance issue in the case of diamond dependencies.

Listing~\ref{y2} shows a \gls{differentiable function} \lstinline{diamondDependentComputationalGraph} that contains diamond dependencies to some \glspl{differentiable expression} or \glspl{trainable variable}.

\begin{lstlisting}[float={h t b p},caption={A diamond dependent \gls{differentiable function}}, label={y2}]
val w = createTrainableVariable(math.random, 0.001)
def diamondDependentComputationalGraph(x: ClosureBasedDualNumber) = {
  val y0 = ClosureBasedDualNumber.multiply(x, w)
  val y1 = ClosureBasedDualNumber.multiply(y0, y0)1
  val y2 = ClosureBasedDualNumber.multiply(y1, y1)
  y2
}
\end{lstlisting}

The \gls{computational graph} of \lstinline{diamondDependentComputationalGraph} are shown in Figure~\ref{diamond}.

\begin{figure}[h t b p]

  \begin{dot2tex}[dot,mathmode]
  digraph {
    node [shape="circle"]
    rankdir=RL
    y_0->x
    y_0->w
    y_1->y_0
    y_1->y_0
    y_2->y_1
    y_2->y_1
  }
  \end{dot2tex}

  \caption{A diamond dependent \gls{computational graph}}
  \label{diamond}
\end{figure}

When \lstinline{y2.backward} is invoked, in order to collect side effects of \lstinline{y2}'s dependencies, \lstinline{y1.backward} will be invoked, twice, and each \lstinline{y1.backward} call will triggers two \lstinline{y0.backward} calls. As a result, for each iteration of backpropagation, \lstinline{y0.backward}, \lstinline{w.backward} and \lstinline{x.backward} are invoked four times, respectively.

The process in \lstinline{y2.backward} is shown in Figure~\ref{diamond backpropagation}.

\begin{figure}[h t b p]
  \newcommand{\x}{$x$}
  \newcommand{\w}{$w$}
  \newcommand{\y}[1]{$y_#1$}
  
  \begin{sequencediagram}
    \newthread{Collecting side effects}{Collecting side effects}
    \newinst{y2}{\y2}
    \newinst{y1}{\y1}
    \newinst{y0}{\y0}
    \newinst{w}{\w}
    \newinst{x}{\x}

    \newcommand{\callyzero}{
      \begin{call}{y1}{\lstinline{y0.backward()}}{y0}{}
        \begin{call}{y0}{\lstinline{x.backward()}}{x}{}
        \end{call}
        \begin{call}{y0}{\lstinline{w.backward()}}{w}{}
        \end{call}
      \end{call}
    }
    
    \newcommand{\callyone}{
      \begin{call}{y2}{\lstinline{y1.backward()}}{y1}{}
        \callyzero
        \callyzero
      \end{call}
    }

    \begin{call}{Collecting side effects}{\lstinline{y2.backward()}}{y2}{}
      \callyone
      \callyone
    \end{call}
  \end{sequencediagram}

  \caption{Backpropagation for a diamond dependent \gls{computational graph}}
  \label{diamond backpropagation}
\end{figure}

Generally, given $n$ levels of nested diamond dependencies, the computational complexity is $O(2^n)$, which is unacceptable for neural networks that may share common \glspl{differentiable expression}.

We introduced reference counting algorithm for dual numbers, to avoid the exponential time complexity.

The reference counting is managed in a wrapper of \lstinline{Tape}, which has additional \lstinline{acquire} and \lstinline{release} functions.

Each wrapper has two internal states:
\begin{enumerate*}
  \item reference counter,
  \item accumulator of delta.
\end{enumerate*}
Respectively, \lstinline{acquire} and \lstinline{release} calls will increase and decrease the reference counter, and \lstinline{backward} calls will cumulate the \lstinline{delta} to the accumulator.

When reference counting algorithm is enabled, \lstinline{backward} is recursive any more. Instead, a wrapper only call \lstinline{backward} of its dependencies when reference counter is decreased to zero. The entire process of backpropagation is shown in Figure~\ref{reference-counted backpropagation}.

\begin{figure}[h t b p]
  \newcommand{\x}{$x$}
  \newcommand{\w}{$w$}
  \newcommand{\y}[1]{$y_#1$}
  
  \begin{sequencediagram}    
    \newthread{Collecting side effects}{Collecting side effects}
    \newinst{y2}{\y2}{}
    \newinst{y1}{\y1}{}
    \newinst{y0}{\y0}{}
    \newinst{w}{\w}{}
    \newinst{x}{\x}{}

    \begin{call}{Collecting side effects}{\lstinline{y2.backward()}}{y2}{}
    \end{call}
    \begin{call}{Collecting side effects}{\lstinline{y2.release() // counter=0}}{y2}{}
      \begin{call}{y2}{\lstinline{y1.backward()}}{y1}{}
      \end{call}
      \begin{call}{y2}{\lstinline{y1.backward()}}{y1}{}
      \end{call}
      \begin{call}{y2}{\lstinline{y1.release() // counter=1}}{y1}{}
      \end{call}
      \begin{call}{y2}{\lstinline{y1.release() // counter=0}}{y1}{}
        \begin{call}{y1}{\lstinline{y0.backward()}}{y0}{}
        \end{call}
        \begin{call}{y1}{\lstinline{y0.backward()}}{y0}{}
        \end{call}
        \begin{call}{y1}{\lstinline{y0.release() // counter=1}}{y0}{}
        \end{call}
        \begin{call}{y1}{\lstinline{y0.release() // counter=0}}{y0}{}
          \begin{call}{y0}{\lstinline{x.backward()}}{x}{}
          \end{call}
          \begin{call}{y0}{\lstinline{w.backward()}}{w}{}
          \end{call}
          \begin{call}{y0}{\lstinline{x.release()}}{x}{}
          \end{call}
          \begin{call}{y0}{\lstinline{w.release()}}{w}{}
          \end{call}
        \end{call}
      \end{call}
    \end{call}
  \end{sequencediagram}

  \caption{Backpropagation for a diamond dependent \gls{computational graph} (with reference counting)}
  \label{reference-counted backpropagation}
\end{figure}

This wrapper is implemented as the monadic data type \href{https://javadoc.io/page/com.thoughtworks.raii/asynchronous_2.11/latest/com/thoughtworks/raii/asynchronous%24%24Do.html}{\lstinline{Do}}, in which the side effects of updating counters and accumulators are monadic control flows. With the help of \lstinline{Do}, now our \glspl{computational graph} are modeled in \lstinline{Do[Tape[Data, Delta]]}, which can be created by \lstinline{forward} methods described in Section~\ref{monadic}. As mentioned in Section~\ref{applicative}, \gls{computational graph} node of binary operations are evaluated in parallel.

In traditional backpropagation implementation, tape is a list, hence both the execution order of backward pass and forward pass must be sequential reverse to each other. Even previous attempt of closure-based tape\cite{pearlmutter2008reverse} still requires conversion to sequential expressions of A-normal form\cite{sabry1993reasoning}. 

By introducing reference counting, the execution order of our backward pass and forward pass do not have to be exactly reverse, hence the conversion to A-normal form becomes unnecessary. As a result, DeepLearning.scala supports out-of-order execution in both forward pass and backward pass, in which the independent sub-graph can be even executed in parallel.

\subsection{The Overview of a Training Iteration}
\label{training iteration}

In brief, in each iteration, a \gls{differentiable function} that contains multiple \glspl{trainable variable} can be trained in the following steps:

\begin{enumerate}
  \item Executing the user-defined \gls{differentiable function} with a batch of input, to obey a \gls{differentiable expression} (i.e. a subtype of \lstinline{Layer}).
  \label{obey}
  
  \item Calling \lstinline{forward} on \gls{differentiable expression} the to build a \gls{computational graph} (i.e. a \lstinline{Do[Tape[Data, Delta]]}), though the reference counter to the \gls{computational graph} is zero at the point.
  \label{build}
  
  \item Performing the \lstinline{forward} pass of \gls{differentiable expression} to build a tape (i.e. a \lstinline{Tape[Data, Delta]}), which contains a pair of the result of forward pass and a \lstinline{backward} closure. The reference counter of each node in \gls{computational graph} are increased during this step.
  \label{acquire}

  \item Performing \lstinline{backward} closure of the root node of the \gls{computational graph}. The accumulator of delta of the root node is stored.

  \item Releasing of the root node of the \gls{computational graph}. The reference counter of each node in \gls{computational graph} are decreased to zero and \lstinline{backward} closure of each node are performed during this step, thus all referenced \glspl{trainable variable} are updated.
  \label{release}

\end{enumerate}

Note that step~\ref{obey} and step~\ref{build} are pure function calls, with no side effects. Step~\ref{acquire} to step~\ref{release} are monadic control flows, which encapsulate some side effects that will be performed only when an unsafe method \lstinline{blockingAwait} or \lstinline{onComplete} is eventually called.

\section{Future Work}

\subsection{New Back-end}
Currently, DeepLearning.scala 2.0's built-in differentiable vector expression type \lstinline{INDArrayLayer} is based on nd4j's \lstinline{INDArray}  \cite{skymind2017nd4j}.
As described in Section~\ref{training iteration}, in each training iteration, for each \gls{computational graph} node, \lstinline{forward} and \lstinline{backward} operations are performed, which internally call some methods on \lstinline{INDArray}, resulting GPU kernel executions if nd4j's CUDA runtime is used. These nd4j operations have a bad computational performance because:
\begin{enumerate*}
  \item Some operations \footnote{\lstinline{INDArray.broadcast} for example} are extremely slow;
  \item Enqueuing a kernel is relatively expensive.
\end{enumerate*}

 We are developing a new back-end as an alternative to nd4j. The new back-end will be able to merge multiple primitive operations into one larger kernel by dynamically generating OpenCL code. The new back-end
 will support more optimized operations on GPU and reduce the number of kernel executions. We expect our new version will achieve better computational efficient.

\subsection{Distributed Model}

Current DeepLearning.scala is only able to run on a standalone JVM, not a distributed cluster, thus it does not support outrageously large neural networks \cite{shazeer2017outrageously} that does not fit into memory of a single node.

Since our \gls{computational graph} are monadic expressions that consist of closures, they can be serialized and executed remotely in theory. We are investigating how to build a distributed machine learning system based on remotely executed monadic expression. We will find out if this suggested approach can support more complex model than the parameter server approach can.




\section{Discussion}

DeepLearning.scala is an unique library among all deep learning frameworks. Our approach of AD has some attributes that never appears in previous frameworks.

\subsection{Interoperable Differentiable Computational Graph}

There were two different mechanisms in state-of-the-art deep learning frameworks:
Define-and-Run v.s. Define-by-Run.

State-of-the-art Define-and-Run frameworks \cite{collobert2008torch,bergstra2010theano,jia2014caffe,chen2015mxnet,abadi2016tensorflow,intel2016bigdl,skymind2017deeplearning4j} allows users to create \glspl{computational graph}, which are immutable Abstract Syntax Trees (ASTs) of some object languages which can be evaluated by the framework runtime. Define-and-Run frameworks can schedule \gls{computational graph} to multiple CPU, GPU or other devices. However, the object languages have bad interoperability with the metalanguage. For example, a DeepLearning4j user cannot use Java control flows nor call Java native methods in neural networks.

State-of-the-art Define-by-Run frameworks \cite{tokui2015chainer,neubig2017dynet,google2017eager,paszke2017pytorch} can eagerly execute actual forward pass calculation in user written code, and, at the same time, generates the internal states for running backward pass. Unlike Define-and-Run frameworks, Define-by-Run frameworks have good interoperability with the hosting language. Control flows and native function calls can be easily used during the execution of neural networks. However, Define-and-Run frameworks tend to store states and perform side effects when defining neural network structures, which makes this mechanism unable to implement in a pure functional flavor.

We discovered the third mechanism of monadic deep learning. Neural networks in DeepLearning.scala are immutable like in Define-and-Run frameworks, and interoperable with Scala like in Define-by-Run frameworks.

\subsection{AD in a Functional Library}

Reverse mode AD in a functional library was previously considered impossible to be implemented without the ability to reflectively access and transform expressions associated with closures \cite{pearlmutter2008reverse}. For example, if you want to create a \lstinline{transform} function that returns the derivative for given function \lstinline{f}:

\begin{lstlisting}[float={h t b p},caption={Impossible transform function for AD}, label={transform}]
def transform(f: Double => Double): Double => Double
\end{lstlisting}

Obviously this \lstinline{transform} function is impossible without the knowledge of the implementation of \lstinline{f}.

Fortunately, in a statically typed language, the differentiable types and non-differentiable types should differ for type safety. The type signature of our AD function can use more powerful type \lstinline{DoubleLayer} instead of \lstinline{Double}. It can be written as Listing~\ref{typeSafeTransform}:

\begin{lstlisting}[float={h t b p},caption={Type safe transform function for AD}, label={typeSafeTransform}]
def typeSafeTransform(f: Double => DoubleLayer) = { input: Double =>
  val tape = f(input).forward
  tape.backward()(Do.now(1.0))
}
\end{lstlisting}

Unlike \cite{pearlmutter2008reverse}'s compiler primitive $\overleftarrow{J}$, our \lstinline{typeSafeTransform} can use the additional methods on \lstinline{DoubleLayer}. As a result, our \lstinline{typeSafeTransform} can be implemented without reflection, as an ordinary Scala function, instead of a compiler primitive or a macro.

\section{Conclusion}

DeepLearning.scala is the first framework that achieves all the following goals:

\begin{itemize}
  \item type safe
  \item pure functional
  \item reverse mode AD
  \item multiple \glspl{trainable variable}
  \item interoperable internal DSL
  \item dynamic neural network
\end{itemize}

With the help of DeepLearning.scala, a normal programmer is able to build complex neural networks from simple code. He still writes code as usual, and the only difference is that the code written in DeepLearning.scala are differentiable, which contains \glspl{trainable variable} that learn knowledges.

\clearpage
% Appendix
\appendix

\printglossary

\begin{acks}
% TODO:
\end{acks}

% Bibliography
\bibliography{bibliography}
