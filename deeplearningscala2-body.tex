\section{Introduction}

Backpropagation\cite{rumelhart1985learning} is the key feature in many deep learning frameworks. Combining with other optimization algorithms\cite{kingma2014adam, zeiler2012adadelta,duchi2011adaptive}, deep learning frameworks change the values of trainable variables in neural networks during iterations, producing a model of knowledge learnt from training data.

Backpropagation can be seen as a special-purpose automatic differentiation\cite{baydin2015automatic}. Many successful Python or Lua deep learning frameworks\cite{collobert2008torch,bergstra2010theano,tokui2015chainer,google2017eager,paszke2017pytorch} implement a common set of features of auto differentiation:

\begin{description}

  \item[Reverse mode] All these deep learning frameworks perform reverse mode automatic differentiation instead of forward mode, as forward mode automatic differentiation does not scale well for deep neural networks.

  \item[Multiple trainable variable] Neural networks are composed of multiple layers. Each layer contains their own trainable variables. All these deep learning frameworks are able to calculus the derivatives of all trainable variables at once for one training data batch.

  \item[Internal DSL\cite{fowler2010domain}] All these deep learning frameworks provide an Internal DSL, allowing users to create their differentiable functions in Python or Lua from the similar expression as creating ordinary non-differentiable functions, and integrate into a larger application alone with higher-level configurations\cite{chollet2015keras} or ETL (Extract, Transform and Load) process.

\end{description}

Unfortunately, deep learning frameworks in statically typed languages have not achieved the above goals until 2017.

Several automatic differentiation libraries\cite{bischof1992adifor,griewank1996algorithm,TapenadeRef13,baydin2015diffsharp} written in Fortran, C++ or F\# support internal DSL via operator overloading but do not support multiple trainable variables.

Other deep learning frameworks in Java or Scala (including Scala binding of C++ frameworks)\cite{intel2016bigdl,skymind2017deeplearning4j,baydin2016hype,chen2017typesafe} do not support automatic differentiation, instead, they only provide their high level computational graph APIs to compose predefined layers into neural networks. As a result, those frameworks do not have the ability to create fine-grained custom algorithms.

In this paper, we present DeepLearning.scala, which achieves all the above goals, and still get type checked.

\section{Basic Concepts\label{concepts}}

For example, suppose we are building a robot for answering questions in IQ test like this:

\begin{quote}
  What is the next number in sequence:
    \begin{quote}
    3, 6, 9, ?
    \end{quote}
  The answer is 12.
\end{quote}

In DeepLearning.scala, the robot can be implemented as a \lstinline{guessNextNumber} function of the following signature\footnote{The code examples from Listing \ref{guessNextNumber} to Listing \ref{predict_trained} do not contain necessary \lstinline{import} and configurations. For a runnable IQ test robot example backed by ND4J\cite{skymind2017nd4j}, see \href{http://deeplearning.thoughtworks.school/demo/GettingStarted.html}{Getting Started documentation on DeepLearning.scala website}.}:

\begin{lstlisting}[caption={The differentiable matrix multiplication implemented by \lstinline{map}/\lstinline{reduce}},label={guessNextNumber}]
def guessNextNumber(question: Seq[Double]): DoubleLayer = {
  // Perform a matrix multiplication between question and weights
  (question zip weights).map {
    case (element, weight) => element * weight
  }.reduce(_ + _) + bias
}
\end{lstlisting}

\lstinline{guessNextNumber} performs a matrix multiplication between question and weights by invoking higher-order functions \lstinline{map} and \lstinline{reduce}. 

Unlike \cite{chen2017typesafe}'s special tensor type, our tensor can be simply typed as \lstinline{Seq[Double]}, \lstinline{Seq[DoubleWeight]} or \lstinline{Seq[DoubleLayer]}.
\footnote{DeepLearning.scala users can use other representations of tensors:
\begin{enumerate*}
  \item For tensors with a statically typed shape, use \href{https://javadoc.io/page/com.chuusai/shapeless_2.11/latest/shapeless/Sized.html}{ \lstinline{shapeless.Sized}}. For example a $10\times20$ two-dimensional tensor can be typed as \lstinline{Sized[Sized[Double, _10], _20]}. For differentiable tensors, replace vanilla \lstinline{Double} to \lstinline{DoubleWeight} or \lstinline{DoubleLayer}.
  \item For GPU-accelerated tensors, use \lstinline{INDArray}\cite{skymind2017nd4j}. For differentiable tensors, use \lstinline{INDArrayLayer} or \lstinline{INDArrayWeight} instead.
\end{enumerate*}
}

The \lstinline{weights} and \lstinline{bias} referenced by \lstinline{guessNextNumber} must be initialized first:

\begin{lstlisting}[caption={Weight initialization}]
val weights: Seq[DoubleWeight] = Stream.continually(DoubleWeight(math.random))
val bias: DoubleWeight = DoubleWeight(math.random)
\end{lstlisting}

Then the robot try to answer IQ test questions like calling an ordinary function:

\begin{lstlisting}[caption={Inference on an untrained model}]
val question = Seq(42.0, 43.0, 44.0)
println(guessNextNumber(question).predict.blockingAwait)
\end{lstlisting}

However, \lstinline{guessNextNumber} returned an incorrect result because the weights and bias were randomly initialized, which have not been trained.

In order to train them, a loss function is necessary:

\begin{lstlisting}[caption={The differentiable square loss function},label={squareLoss}]
def squareLoss(robotAnswer: DoubleLayer, expectedAnswer: Double): DoubleLayer = {
  val difference: DoubleLayer = robotAnswer - expectedAnswer
  difference * difference
}
\end{lstlisting}

The above loss function \lstinline{squareLoss} determines the squared error between robot's answer and the correct answer.

Both \lstinline{squareLoss} and \lstinline{guessNextNumber} are ordinary functions, and can be composed in other functions:

\begin{lstlisting}[caption={A differentiable function to train a linear regression model}]
def linearRegression(question: Seq[Double], expectedAnswer: Double): DoubleLayer = {
  val robotAnswer = guessNextNumber(question)
  squareLoss(robotAnswer, expectedAnswer)
}
\end{lstlisting}

\lstinline{linearRegression}, composed of \lstinline{squaredLoss} and \lstinline{squaredLoss}, returns a \lstinline{DoubleLayer} of the loss for a specific question and its expected answer. \lstinline{linearRegression} is a linear regression model with a square loss, and it can be trained.

\begin{lstlisting}[caption={Training for 500 iterations}]
val question1 = Seq(3.0, 4.0, 5.0)
val expectedAnswer1 = 6.0

val question2 = Seq(13.0, 19.0, 25.0)
val expectedAnswer2 = 31.0

for (iteration <- 0 until 500) {
  linearRegression(question1, expectedAnswer1).train.blockingAwait
  linearRegression(question2, expectedAnswer2).train.blockingAwait
}
\end{lstlisting}

The weights and bias referenced by \lstinline{linearRegression} are modified during 500 iterations of training, toward the direction to minimize the loss returned from \lstinline{linearRegression}.

When weights and bias have been adjusted to make the loss a very small number, \lstinline{guessNextNumber} should return values that are very close to the expected answers.

\begin{lstlisting}[caption={Inference on a trained model},label={predict_trained}]
val question = Seq(42.0, 43.0, 44.0)
println(guessNextNumber(question).predict.blockingAwait)
\end{lstlisting}

This time, it will prints a number closed to 45, because the IQ test robot have finally learnt the pattern of arithmetic progression.

The IQ test robot example shows some basic concepts in DeepLearning.scala.

\begin{itemize}
  \item \lstinline{guessNextNumber}, \lstinline{squareLoss} and \lstinline{linearRegression} are \glspl{differentiable function} that return \glspl{differentiable expression}, which are computational graph nodes that can be evaluated when \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html\#train(differentiable:Differentiable)(implicitmonoid:algebra.ring.MultiplicativeMonoid[DeepLearning.this.Delta]):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{train}}ing or \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html\#predict(differentiable:Differentiable):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{predict}}ing.
  \item \Glspl{differentiable expression} and \glspl{trainable variable} can be used as if they are ordinary non-differentiable values. For example, as shown in Listing \ref{squareLoss}, you can perform scalar subtraction and multiplication between \lstinline{DoubleWeight}, \lstinline{DoubleLayer} and ordinary \lstinline{scala.Double}.
  \item When \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html\#train(differentiable:Differentiable)(implicitmonoid:algebra.ring.MultiplicativeMonoid[DeepLearning.this.Delta]):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{train}}ing a \gls{differentiable expression}, it returns a \href{https://javadoc.io/page/com.thoughtworks.future/future_2.11/latest/com/thoughtworks/future$$Future.html}{\lstinline{Future}}, which encapsulates the side-effect of adjusting \glspl{trainable variable} referenced by the \gls{differentiable function}.
  \item If a \gls{differentiable function} invokes another \gls{differentiable function}, then \glspl{trainable variable} trained by one \gls{differentiable function} affect another one. For example, when training the \gls{differentiable function} \lstinline{linearRegression}, The \glspl{trainable variable} \lstinline{weights} and \lstinline{bias} are modified, hence \lstinline{guessNextNumber} automatically gains the ability to \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html\#predict(differentiable:Differentiable):com.thoughtworks.future.Future[DeepLearning.this.Data]}{\lstinline{predict}} correct answers.
\end{itemize}

\section{Dynamic Neural Networks}

DeepLearning.scala supports dynamic neural network. It means that the control flow of a neural network can differ according to its internal intermediate state when processing a special input. Especially, the ability to conditional enabling a sub-neural network is a crucial feature to build outrageously large neural networks\cite{shazeer2017outrageously}.

Suppose we have two sub-neural networks, \lstinline{leftSubnet} and \lstinline{rightSubnet}. We want to build a gated network, which conditionally runs either \lstinline{leftSubnet} or \lstinline{rightSubnet} for a special \lstinline{input}.

\begin{lstlisting}[caption={Predefined sub-networks}]
def leftSubnet(input: INDArrayLayer): INDArrayLayer
def rightSubnet(input: INDArrayLayer): INDArrayLayer
\end{lstlisting}

Which sub-network is selected for the \lstinline{input} should be determined by the \lstinline{gate} network, which returns a pair of differentiable double expressions that indicate the preferences between \lstinline{leftSubnet} and \lstinline{rightSubnet}.

\begin{lstlisting}[caption={Predefined gate network}]
def gate(input: INDArrayLayer): (DoubleLayer, DoubleLayer)
\end{lstlisting}

The control flow of gated network that we want to build is described in pseudo-code \ref{GatedNetwork}.

\begin{function}[H]
  \caption{GatedNetwork()\label{GatedNetwork}}
  \KwIn{Features extracted by preceding layers}
  \KwOut{Features passed to succeeding layers}
  \SetKwFunction{gate}{gate}
  \SetKwFunction{leftSubnet}{leftSubnet}
  \SetKwFunction{rightSubnet}{rightSubnet}
  scores $\leftarrow$ \gate{Input}\;
  \eIf{score of left sub-network $>$ score of right sub-network}{
      \KwRet{score of left sub-network $\times$ \leftSubnet{Input}}\;
  }{
      \KwRet{score of right sub-network $\times$ \rightSubnet{Input}}\;
  }
\end{function}

In DeepLearning.scala, there are three different approaches to implement the gated network. Examples of these approaches are introduced in following section \ref{eager}, \ref{monadic}, and \ref{applicative}.

\subsection{Eager Execution (bad)\label{eager}}

An obvious approach (Listing \ref{naiveGatedNet}) to create the gated network is eagerly executing the \lstinline{gate}.

\begin{lstlisting}[caption={The eager execution implementation of gated network}, label={naiveGatedNet}]
def naiveGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  if (scores._1.predict.blockingAwait > scores._2.predict.blockingAwait) {
    scores._1 * leftSubnet(input)
  } else {
    scores._2 * rightSubnet(input)
  }
}
\end{lstlisting}

There are three sub-networks in the \lstinline{naiveGatedNet} function. The \lstinline{gate} returns a pair of \lstinline{DoubleLayer}s. By blocking await the \lstinline{predict}ion result, we got two \lstinline{Double}s, which can be used to determine which sub-network is preferred between \lstinline{leftSubnet} and \lstinline{rightSubnet}. The chosen sub-network will multiplies with the value returned by the \lstinline{gate} in order to enabling backpropagation on the \lstinline{gate}.

However, there is a performance issue in the \lstinline{naiveGatedNet}.

In DeepLearning.scala, all differentiable expressions, including the scalar \lstinline{DoubleLayer} and vectorize \lstinline{INDArrayLayer}, represent lazily evaluated differentiable computational graph nodes, which will not be executed until their predict or train methods are invoked.

So, the two calls to the \lstinline{predict} method in the \lstinline{if} will execute the computational graph of \lstinline{gate} twice. Also \lstinline{naiveGatedNet} itself is a computational graph. There will be a \lstinline{predict} or \lstinline{train} call on \lstinline{naiveGatedNet} from user in the future, which will execute the computational graph one more time. But what's even worse is, \lstinline{input} is a computational graph, too. Along with \lstinline{gate}, it will be evaluated three times, which may contain complex future extracting process.

\subsection{Monadic Control Flow (good)\label{monadic}}

Ideally, the calls to \lstinline{predict} should be avoided in differentiable functions. The recommended approach to create a dynamic neural network is using \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html\#forward(differentiable:Differentiable):com.thoughtworks.raii.asynchronous.Do[com.thoughtworks.deeplearning.DeepLearning.Tape[DeepLearning.this.Data,DeepLearning.this.Delta]]}{\lstinline{forward}}, which returns a monadic value of \lstinline{Do[Tape[Data, Delta]]}, which can be used in a monadic control flow via Scalaz\cite{kenji2017scalaz}'s type classes\cite{oliveira2010type} \lstinline{Monad} and \lstinline{Applicative}.

Listing \ref{monadicGatedNet} shows the monadic control flow of gated network.

\begin{lstlisting}[caption={Monadic gated network}, label={monadicGatedNet}]
def monadicGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  val gatedForward: Do[Tape[INDArray, INDArray]] = {
    scores._1.forward.flatMap { tape1: Tape[Double, Double] =>
      scores._2.forward.flatMap { tape2: Tape[Double, Double] =>
        if (tape1.data > tape2.data) {
          (scores._1 * leftSubnet(input)).forward
        } else {
          (scores._2 * rightSubnet(input)).forward
        }
      }
    }
  }
  INDArrayLayer(gatedForward)
}
\end{lstlisting}

This gated network is built from the monadic expression \lstinline{gatedForward}, which contains some \lstinline{forward} calls, which are asynchronous operations (or \href{https://javadoc.io/page/com.thoughtworks.raii/asynchronous_2.11/latest/com/thoughtworks/raii/asynchronous$$Do.html}{\lstinline{Do}}) that produce Wengert list record (or \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning$$Tape.html}{\lstinline{Tape}}). The implementation detail of \lstinline{Do} and \lstinline{Tape} will be discussed in section \ref{backpropagation}. For now, we only need to know that \lstinline{Do} is a monadic data type that supports \lstinline{flatMap}. By \lstinline{flatMap}ping those \lstinline{forward} operations together, we built the entire monadic control flow \lstinline{gatedForward} for the gated network.

The \lstinline{monadicGatedNet} represents a dynamic neural network, since each \lstinline{forward} operation is started after its previous \lstinline{forward} done. This behavior allows dynamically determining succeeding operations according to result of previous \lstinline{forward} operations, as shown in the \lstinline{if} clause in Listing \ref{monadicGatedNet}.

However, \lstinline{flatMap} prevents additional optimization, too.
\lstinline{scores._2.forward} have to wait for \lstinline{scores._1.forward}'s result, even if the two operations are logically independent.

\subsection{Parallel Applicative Control Flow + Sequential Monadic Control Flow (best)\label{applicative}}

Ideally, the independent operations \lstinline{scores._1.forward} and \lstinline{scores._2.forward} should run in parallel. This can be done by tagging \lstinline{Do} as \href{https://javadoc.io/page/org.scalaz/scalaz_2.11/latest/scalaz/Tags$$Parallel.html}{\lstinline{Parallel}}, and use \href{https://javadoc.io/page/org.scalaz/scalaz_2.11/latest/scalaz/Applicative.htm\l#tuple2[A,B](fa:=>F[A],fb:=>F[B]):F[(A,B)]}{\lstinline{scalaz.Applicative.tuple2}} instead of \lstinline{flatMap} (Listing \ref{applicativeMonadicGatedNet}).

\begin{lstlisting}[caption={Applicative + monadic gated network}, label={applicativeMonadicGatedNet}]
def applicativeMonadicGatedNet(input: INDArrayLayer): INDArrayLayer = {
  val scores = gate(input)
  val parallelForward1: ParallelDo[Tape[Double, Double]] = {
    Parallel(scores._1.forward)
  }
  val parallelForward2: ParallelDo[Tape[Double, Double]] = {
    Parallel(scores._2.forward)
  }
  val Parallel(stage1) = {
  	parallelForward1.tuple2(parallelForward2)
  }
  def stage2(tapes: (Tape[Double, Double], Tape[Double, Double])) = {
    if (tapes._1.data > tapes._2.data) {
      (scores._1 * leftSubnet(input)).forward
    } else {
      (scores._2 * rightSubnet(input)).forward
    }
  }

  val gatedForward = stage1.flatMap(stage2)
  INDArrayLayer(gatedForward)
}
\end{lstlisting}

This \lstinline{applicativeMonadicGatedNet} takes both advantages from applicative functors and monads. The entire control flow is a \lstinline{flatMap} sequentially composed of two stages. In \lstinline{stage1}, there is a \lstinline{tuple2} composed of \lstinline{scores._1.forward} and \lstinline{scores._2.forward} in parallel. Then, in \lstinline{stage2}, the succeeding operation is dynamically determined according to \lstinline{tapes}, the result of \lstinline{stage1}.

The parallel applicative operation is also the default behavior for all built-in vector binary operators. Listing \ref{parallelByDefault} shows some simple expressions that will be executed in parallel.

\begin{lstlisting}[caption={By default, \lstinline{a * b} and \lstinline{c * d} will be executed in parallel because they are independent}, label={parallelByDefault}]
def parallelByDefault(a: INDArrayLayer, b: INDArrayLayer, c: INDArrayLayer, d: INDArrayLayer): INDArrayLayer = {
  a * b + c * d
}
\end{lstlisting}

By combining both applicative functors and monads, DeepLearning.scala supports dynamic neural network and still allows the independent parts of the neural network to run in parallel. In addition, the backward pass of differentiable functions built from parallel applicative functors or built-in vector binary operators will be executed in parallel, too.

\section{Hyperparameters}

\subsection{Configuring Hyperparameters with Plug-ins}

Nowadays neural networks contain hyperparameters specified by human. In DeepLearning.scala, those hyperparameters are configured as named parameters provided by \glspl{plug-in}.

\footnote{In Ammonite REPL\cite{lihaoyi2017ammonite} or Jupyter Scala\cite{alexarchambault2017jupyter}, the hyperparameter initialization code should be wrapped in a \lstinline{interp.load} call, as a workaround for \url{https://github.com/lihaoyi/Ammonite/issues/649} and \url{https://github.com/scala/bug/issues/10390}.}

\begin{lstlisting}[caption={Configuring hyperparameters with a fixed learning rate}, label={hyperparameters1}]
val hyperparameters1 = Factory[Builtins with FixedLearningRate].newInstance(learningRate = 0.01)
\end{lstlisting}

\lstinline{Builtins} and \lstinline{FixedLearningRate} are \glspl{plug-in}. The name parameters like \lstinline{learningRate} may may differ when different \glspl{plug-in} are used.

\Glspl{differentiable expression} and \glspl{trainable variable} introduced in section \ref{concepts} are actually path-dependent types\cite{amin2014foundations} defined inside \lstinline{hyperparameter1}, whose type is a refinement type of \lstinline{(Builtins with FixedLearningRate)}\footnote{We omit the type annotation for \lstinline{hyperparameter1} in this paper because the refinement type is too long. Add a \lstinline{-Xprint:typer} flag to your Scala compiler if you want to see the actual type.}, which can be \lstinline{import}ed as shown in Listing \ref{import_types}.

\begin{lstlisting}[caption={Importing \Glspl{differentiable expression} and \glspl{trainable variable} types}, label={import_types}]
import hyperparameters1.{INDArrayLayer, INDArrayWeight}
\end{lstlisting}

Then we can create \lstinline{hyperparameters1.INDArrayLayer} and \lstinline{hyperparameters1.INDArrayWeight}, shown in Listing \ref{weight1} and \ref{layer1}.

\begin{lstlisting}[caption={\Glspl{trainable variable} that belong to \lstinline{hyperparameter1}}, label={weight1}]
val weight1: INDArrayWeight = INDArrayWeight(Nd4j.randn(10, 10))
\end{lstlisting}

The learning rate of \lstinline{weight1} should be 0.01 according to configuration in Listing \ref{hyperparameters1}.

\begin{lstlisting}[caption={\Glspl{differentiable expression} that belong to \lstinline{hyperparameter1}}, label={layer1}]
def layer1(input: INDArrayLayer): INDArrayLayer = {
  import hyperparameters1.implicits._
  import hyperparameters1.max
  max(input.dot(weight1), 0.0)
}
\end{lstlisting}

Differentiable functions \lstinline{max} and \lstinline{dot} used in \lstinline{layer1} is also defined inside \lstinline{hyperparameters1}, hence the \lstinline{import} statements for them are necessary.

\subsection{Heterogeneous Hyperparameters}

DeepLearning.scala allows \glspl{differentiable expression} that belong to one hyperparameter configuration to work together with \glspl{differentiable expression} that belong to another hyperparameter configuration. 

\begin{lstlisting}[caption={Hyperparameters for Adagrad}, label={hyperparameters2}]
val hyperparameters2 = Factory[Builtins with Adagrad with FixedLearningRate].newInstance(
  learningRate = 0.002,
  eps = 0.0001
)
\end{lstlisting}

\begin{lstlisting}[caption={\Glspl{trainable variable} optimized by Adagrad}, label={weight2}]
val weight2 = hyperparameters2.INDArrayWeight(Nd4j.randn(10, 10))
\end{lstlisting}

\begin{lstlisting}[caption={A two-layer neural network built from heterogeneous hyperparameters}, label={layer2}]
def layer2(input: hyperparameters1.INDArrayLayer): hyperparameters2.INDArrayLayer = {
  import hyperparameters2.implicits._
  import hyperparameters2.max
  
  /* This import is necessary for hyperparameters1.INDArrayLayer.dot() method. It is Renamed in case of name clash with hyperparameters2.implicits._ */
  import hyperparameters1.implicits.{
  	layerDeepLearning => layerDeepLearning1
  }
  
  max(layer1(input).dot(weight2), 0.0)
}
\end{lstlisting}

Listing \ref{layer2} created the two-layer neural network \lstinline{layer2} that has heterogeneous hyperparameters for each layer:

\begin{itemize}
  \item \lstinline{weight1} is optimized by ordinary gradient descent with a fixed learning rate 0.1;
  \item \lstinline{weight2} is optimized by Adagrad\cite{duchi2011adaptive} with an initial learning rate 0.002.
\end{itemize}

\subsubsection{Ad Hoc Polymorphism}

However, the type signature of \lstinline{layer2} seems not very usable. It returns a \lstinline{hyperparameters2.INDArrayLayer} but only accepts a \lstinline{hyperparameters1.INDArrayLayer} as input, which means that the previous layer of \lstinline{layer2} must belong to \lstinline{hyperparameters1}.

Ideally, \lstinline{layer1} and \lstinline{layer2} should be polymorphic differentiable functions, able to accept heterogeneous types of input for better reusability, including:

\begin{itemize}
  
  \item A vanilla vector input. i.e. \lstinline{INDArray}.

  \item \Glspl{differentiable expression} of hidden states produced by any previous neural network layers, regardless of what hyperparameters are configured for those layers. i.e. any \lstinline{INDArrayLayer}s regardless of the prefixes.
  
  \item \Glspl{trainable variable} in the case of activation maximization technique\cite{erhan2009visualizing}.i.e. any \lstinline{INDArrayWeight}s regardless of the prefixes.

\end{itemize}

We provide a dependent-type type class\cite{gurnelltype} \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/DeepLearning.html}{\lstinline{DeepLearning}} that witnesses any supported expressions including \glspl{differentiable expression}, \glspl{trainable variable}, or vanilla non-differentiable types. The users can create type aliases to restrict the types of state during forward pass and backward pass by refining  as shown in Listing \ref{INDArrayDeepLearning}.

\begin{lstlisting}[caption={A type class alias that witnesses dense vector expressions}, label={INDArrayDeepLearning}]
type INDArrayExpression[Expression] = DeepLearning[Expression] {
  /** The type of result calculated during forward pass */
  type Data = INDArray

  /** The type of derivative during backward pass */
  type Delta = INDArray
}
\end{lstlisting}



By using \lstinline{INDArrayExpression} as a context bound, \lstinline{layer1} and \lstinline{layer2} can accept any vector expression as shown in Listing \ref{polyLayer2}.

\begin{lstlisting}[caption={Polymorphic two-layer neural network}, label={polyLayer2}]
def polyLayer1[Input: INDArrayExpression](input: Input): hyperparameters1.INDArrayLayer = {
  import hyperparameters1.implicits._
  import hyperparameters1.max
  max(input.dot(weight1), 0.0)
}

def polyLayer2[Input: INDArrayExpression](input: Input): hyperparameters2.INDArrayLayer = {
  import hyperparameters2.implicits._
  import hyperparameters2.max
  import hyperparameters1.implicits.{layerDeepLearning => layerDeepLearning1}
  max(polyLayer1(input).dot(weight2), 0.0)
}
\end{lstlisting}

Built-in operations like \lstinline{max} and \lstinline{dot} can be used as before, because all built-in operations are polymorphic, too.

With the help of \lstinline{DeepLearning} type class, we created a two-layer neural network \lstinline{polyLayer2} that
\begin{enumerate*}
  \item has heterogeneous hyperparameters, and
  \item accepts polymorphic types of input. 
\end{enumerate*}

\subsection{Built-in Plug-ins at a Glance}

DeepLearning.scala's \gls{plug-in} mechanism is a solution to the well known Expression Problem, whose goal is to define a datatype by cases, where one can add new cases to the datatype and new functions over the datatype, without recompiling existing code, and while retaining static type safety (e.g., no casts)\cite{wadler1998expression}.

All DeepLearning.scala features, including types and operations, are implemented as \glspl{plug-in}. The \lstinline{Builtins} \gls{plug-in} used in Listing \ref{hyperparameters1} and \ref{hyperparameters2} is simply a shortcut of many other built-in \glspl{plug-in}.

For example, hyperparameter initialization in Listing \ref{hyperparameters1} is synonymous with the following Listing \ref{hyperparameters1_alternative}.

\begin{lstlisting}[caption={Configuring hyperparameters with a fixed learning rate, alternatively}, label={hyperparameters1_alternative}]
val hyperparameters1 = Factory[ImplicitsSingleton with Layers with Weights with Logging with Names with Operators with FloatTraining with FloatLiterals with FloatWeights with FloatLayers with CumulativeFloatLayers with DoubleTraining with DoubleLiterals with DoubleWeights with DoubleLayers with CumulativeDoubleLayers with INDArrayTraining with INDArrayLiterals with INDArrayWeights with INDArrayLayers with CumulativeINDArrayLayers with FixedLearningRate
].newInstance(learningRate = 0.01)
\end{lstlisting}

All those \glspl{plug-in} are optional. They provide either some special \textit{data types}, \textit{cases} or \textit{functions} as described in the original Expression Problem.

\begin{description}
\item[data types] For example, if you remove \lstinline{FloatTraining}, \lstinline{FloatLiterals},  \lstinline{FloatWeights}, \lstinline{FloatLayers} and \lstinline{CumulativeFloatLayers} from Listing \ref{hyperparameters1_alternative}, then all types for single precision floating-points, such as  \lstinline{FloatLayer} or \lstinline{FloatWeight} will no longer exist in \lstinline{hyperparameters1}, so do the operations for them.
\item[cases] However, if you only remove \lstinline{FloatLayers} and {CumulativeFloatLayers} from Listing \ref{hyperparameters1_alternative}, then \lstinline{max}, \lstinline{dot} and other arithmetic operations will no longer exist in \lstinline{hyperparameters1}. Users can also create other \glspl{plug-in} for additional cases, \href{https://gist.github.com/Atry/15b7d9a4c63d95ad3d67e94bf20b4f69#file-readme-ipynb}{Convolutional Operations} for example.
\item[functions] Most of \glspl{plug-in} are designed for the functionality related to automatically differentiation, however, \lstinline{Names} \gls{plug-in} is not one of them. If you remove \lstinline{Names} from Listing \ref{hyperparameters1_alternative}, then the public methods \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/plugins/Names$Layer.html#name:sourcecode.Name}{\lstinline{name}} and \href{https://javadoc.io/page/com.thoughtworks.deeplearning/deeplearning_2.11/latest/com/thoughtworks/deeplearning/plugins/Names$Layer.html#fullName:sourcecode.FullName}{\lstinline{fullName}} will disappear, while the mechanism of automatically differentiation will still keep working.
\end{description}


\subsection{Creating Plug-ins}

What makes our \gls{plug-in} extraordinary in comparison to original Expression Problem is the ability to modify existing behaviors. Let's see the implementation of \lstinline{Adagrad} \gls{plug-in} as an example
\begin{anonsuppress}
  \footnote{\lstinline{Adagrad} is a non-built-in \gls{plug-in}, which can be found on Github Gist: \url{https://gist.github.com/Atry/89ee1baa4c161b8ccc1b82cdd9c109fe\#file-readme-ipynb}}
\end{anonsuppress}
:

\begin{lstlisting}[caption={The implementation of the \lstinline{Adagrad} \gls{plug-in}}, label={Adagrad}]
trait Adagrad extends INDArrayWeights {
  /** The hyperparameter `eps`, which should be configured in `Factory.newInstance()` */
  def eps: Double

  trait INDArrayWeightApi extends super.INDArrayWeightApi { this: INDArrayWeight =>
    /** The `cache` state injected to `INDArrayWeight` by this `Adagrad` plug-in */
    var cache: Option[INDArray] = None
  }

  override type INDArrayWeight <: INDArrayWeightApi with Weight

  trait INDArrayOptimizerApi extends super.INDArrayOptimizerApi { this: INDArrayOptimizer =>
    private lazy val delta0: INDArray = {
      // The original delta computed by previous plug-ins
      val superDelta = super.delta
      val newCache = weight.synchronized {
        val newCache = weight.cache.getOrElse(Nd4j.zeros(superDelta.shape: _*)) + superDelta * superDelta
        weight.cache = Some(newCache)
        newCache
      }
      superDelta / (Transforms.sqrt(newCache) + eps)
    }

    /** The computation of delta, injected to `INDArrayOptimizer` by this `Adagrad` plug-in */
    override def delta = delta0
  }
  
  override type INDArrayOptimizer <: INDArrayOptimizerApi with Optimizer
}
\end{lstlisting}

The implementation of \lstinline{Adagrad} \gls{plug-in} consists of:

\begin{itemize}
  \item Declaring the \lstinline{INDArrayWeights} \gls{plug-in} as a dependency.
  \item Adding a new hyperparameter \lstinline{eps}, which can be configured by a named parameter in \lstinline{Factory.newInstance()}.
  \item Changing the upper type bound of abstract type \lstinline{INDArrayWeight} to \lstinline{INDArrayWeightApi}.
  \begin{itemize}
    \item Adding a new internal state \lstinline{cache} in \lstinline{INDArrayWeightApi}.
  \end{itemize}
  \item Changing the upper type bound of abstract type \lstinline{INDArrayOptimizer} to \lstinline{INDArrayOptimizerApi}.
  \begin{itemize}
    \item Changing the behavior of \lstinline{delta} method \lstinline{cache} in \lstinline{INDArrayOptimizerApi}.
  \end{itemize}
\end{itemize}

Thanks to Scala's Class Linearization\cite{odersky2017scala} feature, \lstinline{Adagrad} \gls{plug-in} can change the computation of \lstinline{delta} without the knowledge of which \glspl{plug-in} other than \lstinline{Adagrad} are changing \lstinline{delta}, too. This ability is even more extensible than the original Expression Problem\begin{anonsuppress}\cite{yang2017expression}
\end{anonsuppress}
.



\section{Backpropagation through Monads\label{backpropagation}}
% TODO:
\section{Conclusion}
\subsection{Future Work}

\clearpage
% Appendix
\appendix

\printglossary

\begin{acks}
% TODO:
\end{acks}

% Bibliography
\bibliography{deeplearningscala2}
